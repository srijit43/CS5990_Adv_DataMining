# -------------------------------------------------------------------------
# AUTHOR: Srijit Bhattacharya
# FILENAME: bagging_random_forest.py
# SPECIFICATION: The goal is to build a base classifier by using a single
# decision tree, an ensemble classifier that combines multiple decision trees,
# and a Random Forest classifier to recognize those digits.
# FOR: CS 4210- Assignment #3
# TIME SPENT: 100 minutes
# -----------------------------------------------------------*/
from sklearn import tree
from sklearn.utils import resample
from sklearn.ensemble import RandomForestClassifier
import csv

db_training = []
db_test = []
X_training = []
Y_training = []
class_votes = []

# reading the training data in a csv file
with open('optdigits.tra', 'r') as trainingFile:
    for i, row in enumerate(csv.reader(trainingFile)):
        db_training.append(row)

# reading the test data in a csv file
with open('optdigits.tes', 'r') as testingFile:
    for i, row in enumerate(csv.reader(testingFile)):
        db_test.append(row)
        # inititalizing the class votes for each test sample
        class_votes.append([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

    print("Started my base and ensemble classifier ...")

    for k in range(20):  # we will create 20 bootstrap samples here (k = 20). One classifier will be created for each bootstrap sample

        bootstrapSample = resample(
            db_training, n_samples=len(db_training), replace=True)

        # populate the values of X_training and Y_training by using the
        # bootstrapSample
        X_training = [sample[:-1] for sample in bootstrapSample]
        Y_training = [sample[-1] for sample in bootstrapSample]

        # fitting the decision tree to the data
        # we will use a single decision tree without pruning it
        clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=None)
        clf = clf.fit(X_training, Y_training)

        correct = 0
        for i, test_sample in enumerate(db_test):
            # make the classifier prediction for each test sample and update
            # the corresponding index value in classVotes. For instance,
            # if your first base classifier predicted 2 for the first test
            # sample, then classVotes[0,0,0,0,0,0,0,0,0,0] will change to
            # classVotes[0,0,1,0,0,0,0,0,0,0].
            # Later, if your second base classifier predicted 3 for the first
            # test sample, then classVotes[0,0,1,0,0,0,0,0,0,0] will change to classVotes[0,0,1,1,0,0,0,0,0,0]
            # Later, if your third base classifier predicted 3 for the first
            # test sample, then classVotes[0,0,1,1,0,0,0,0,0,0] will change to
            # classVotes[0,0,1,2,0,0,0,0,0,0]
            # this array will consolidate the votes of all classifier for all
            # test samples
            y_test = test_sample[-1]
            prediction = clf.predict(X=[test_sample[:-1]])[0]
            class_votes[i][int(prediction)] += 1

            # for only the first base classifier, compare the prediction with
            # the true label of the test sample here to start calculating its
            # accuracy
            if k == 0:
                correct += 1 if y_test == prediction else 0

        # for only the first base classifier, print its accuracy here
        if k == 0:
            accuracy = correct / (len(db_test) + 1)
            print("Finished my base classifier (fast but relatively low accuracy) ...")
            print("My base classifier accuracy: " + str(accuracy))
            print("")

    # now, compare the final ensemble prediction (majority vote in classVotes)
    # for each test sample with the ground truth label to calculate the
    # accuracy of the ensemble classifier (all base classifiers together)
    correct = 0
    for i, sample_votes in enumerate(class_votes):
        # get the index of the max element in test sample votes array this
        # is the class we will choose and compare it to the class of the test
        # sample
        class_predicated_ensemble = sample_votes.index(max(sample_votes))
        if class_predicated_ensemble == int(db_test[i][-1]):
            correct += 1

    # printing the ensemble accuracy here
    accuracy = correct / (len(db_test) + 1)
    print("Finished my ensemble classifier (slow but higher accuracy) ...")
    print("My ensemble accuracy: " + str(accuracy))
    print("")

    print("Started Random Forest algorithm ...")

    # Create a Random Forest Classifier
    # this is the number of decision trees that will be generated by Random
    # Forest. The sample of the ensemble method used before
    clf = RandomForestClassifier(n_estimators=20)

    # Fit Random Forest to the training data
    clf.fit(X_training, Y_training)

    # make the Random Forest prediction for each test sample. Example:
    # class_predicted_rf = clf.predict([[3, 1, 2, 1, ...]]
    correct = 0
    for i, test_sample in enumerate(db_test):
        y_test = test_sample[-1]
        class_predicted_rf = clf.predict([test_sample[:-1]])[0]
        if class_predicted_rf == y_test:
            correct += 1

    # printing Random Forest accuracy here
    accuracy = correct / (len(db_test) + 1)
    print("Random Forest accuracy: " + str(accuracy))
    print("Finished Random Forest algorithm (much faster and" +
          " higher accuracy!).")
